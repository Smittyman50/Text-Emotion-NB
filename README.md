# Text-Emotion-NB

This directory contains a complete implementation of a text-based emotion classification model that uses Natural Language Processing (NLP) techniques to identify emotions from written text.

The model applies TF–IDF (Term Frequency–Inverse Document Frequency) feature extraction and a Multinomial Naive Bayes (NB) classifier to transform and learn from linguistic patterns associated with emotional tone.

The objective of this project is to design a transparent, interpretable, and reproducible NLP workflow capable of distinguishing multiple emotional categories from text data with strong generalization and balanced class performance.

All processes—from data preparation and exploratory analysis to model training, evaluation, and promotion—are automated within a structured experiment framework that records every artifact for auditability and future optimization.

## Notebooks

### `01_prepare_dataset.ipynb`

This notebook handles all data preprocessing, cleaning, and feature preparation for textual emotion analysis.

Its primary functions and features include:

- **Data ingestion**: loads the raw text dataset from the designated source folder `./data/data_raw/`, containing sentence-level records labeled by emotion class.
- **Text hygiene**: removes null or empty text, trims whitespace, lowercases all content, and filters out duplicate entries within each class to prevent training bias.
- **Normalization and token safety**: applies regular expressions to remove special characters, punctuation, and non-linguistic artifacts, using the `regex` and `unicodedata2` packages for Unicode-safe normalization.
- **Dataset balancing**: optionally stratifies or downsamples the data to mitigate strong class imbalance (controlled by parameters in the notebook).
- **Feature readiness**: merges text fields if multiple columns exist (e.g., “Summary” and “Text”), preparing a single input column for vectorization.
- **Artifact creation**:
  - `clean.csv` – the finalized preprocessed dataset stored in `./data/data_clean/`
  - `label_map.json` – numeric-to-emotion label mapping for consistent decoding
  - `manifest.json` – record of row counts, source file, and creation timestamp
- **Reusability**: this notebook does not perform modeling; it provides a clean dataset ready for TF–IDF vectorization and classification.

### `TextEmotionNB.ipynb`

This notebook defines, trains, and evaluates the text-based emotion classifier built around the TF–IDF + Multinomial Naive Bayes pipeline.

It follows a modular and reproducible structure similar to my first model.

Its primary functions and features include:

- **Environment setup**: defines the directory constants (`DATA`, `SPLIT_DIR`, `ART`, `MODELS`, `TRIALS_ROOT`, `OUTPUTS_DIR`) and verifies that all required folders exist for saving artifacts and outputs.
- **Dataset loading**: loads the preprocessed dataset (`clean.csv`) generated by the data preparation notebook and the associated label mapping (`label_map.json`) to ensure consistent numeric label encoding across runs.
- **Feature extraction**: builds and fits a TF–IDF vectorizer using tunable parameters (`max_features`, `ngram_range`, `min_df`, `max_df`, `sublinear_tf`) to transform cleaned text into a weighted term-frequency representation suitable for probabilistic classification.
- **Model definition and training**: initializes and trains a Multinomial Naive Bayes classifier on the TF–IDF features; uses `joblib` for persistence and aligns with scikit-learn’s native interfaces for evaluation and inference compatibility.
- **Evaluation and metrics**: evaluates model performance on the test set and computes accuracy, precision, recall, and macro-averaged F1.
Generates both raw and normalized confusion matrices (`confusion_matrix_raw.png`, `confusion_matrix_norm.png`) and writes detailed evaluation summaries to `classification_report.txt` and `metrics.json`.
- **Experiment tracking**: records each training run under a unique timestamped directory within `./artifacts/trials/<run_id>/`, storing the model artifacts (`nb.joblib`, `tfidf.joblib`) along with metrics, visualizations, and metadata.
Appends each run’s summary metrics to `./artifacts/outputs/leaderboard.csv` for transparent comparison across experiments.
- **Model promotion**: automatically selects the top-performing run by sorting `leaderboard.csv` on `test_macro_f1` and copies the corresponding artifacts (`nb.joblib`, `tfidf.joblib`, and `metrics.json`) into `./artifacts/models/` as `best_nb.joblib`, `best_tfidf.joblib`, and `best_metrics.json`.
- **Reproducibility**: enforces deterministic execution by seeding Python, NumPy, and scikit-learn random states; preserves all run metadata and metrics to enable future retraining and audit verification.

## Project Structure

```
Text-Emotion-NB/
├── artifacts/
│   ├── models/
│   │   ├── best_metrics.json
│   │   ├── best_nb.joblib
│   │   └── best_tfidf.joblib
│   ├── outputs/
│   │   ├── eda.json
│   │   ├── label_map.json
│   │   ├── leaderboard.csv
│   │   └── manifest.json
│   └── trials/
│       └── <run_id>/
│           ├── classification_report.txt
│           ├── confusion_matrix_norm.png
│           ├── confusion_matrix_raw.png
│           ├── metrics.json
│           ├── nb.joblib
│           ├── per_class_scores.png
│           ├── tfidf.joblib
│           ├── top_missclassifications.csv
│           ├── top_words_per_class.json
│           ├── training_metadata.json
│           ├── wordcloud_0.png
│           ├── wordcloud_1.png
│           ├── wordcloud_2.png
│           └── ...                    # continues for all numeric labels
├── data/
│   ├── data_clean/     # cleaned dataset lives here
│   │   └── clean.csv
│   └── data_raw/       # original dataset from Kaggle lives here
├── notebooks/
│   ├── 01_prepare_dataset.ipynb
│   └── TextEmotionNB.ipynb
├── README.md
└── requirements.txt
```

## Dependencies

### Required

Python 3.13 (virtual environment recommended)

- `pandas` – data loading, cleaning, and export of the preprocessed dataset
- `numpy` – numerical arrays and intermediate matrix work
- `scikit-learn` – TF–IDF vectorization, MultinomialNB model, and evaluation metrics
- `scipy` – utilities required by scikit-learn for sparse operations
- `matplotlib` – confusion matrix plot generation
- `seaborn` – optional styled plots and data exploration
- `regex` – robust text hygiene and pattern-based cleanup
- `unicodedata2` – Unicode normalization for diverse text sources
- `joblib` – persistence for nb.joblib and tfidf.joblib
- `orjson` – fast JSON for metrics/metadata (as used in your notebook)
- `pathlib` – path-safe filesystem ops
- `tqdm` – progress indicators
- `wordcloud` – optional text visualization

Install from `requirements.txt` using:

```
pip install -r requirements.txt
```

## Usage

1. **Clone or download the repository to your local environment.**

2. **Launch VS Code and ensure the virtual environment is activated in a terminal window.**
   
    ```
    python -m venv .venv
    .venv\Scripts\Activate.ps1
    ```

3. **Install dependencies as described (Dependencies section)**

4. **Download the dataset from https://www.kaggle.com/datasets/bhavikjikadara/emotions-dataset and extract it into the `./data/data_raw/` directory.**

5. **Run data preparation**
    - Open `02_prepare_dataset.ipynb`.
    - Execute all cells to clean and standardize the dataset.
    - Confirm that `clean.csv`, `label_map.json`, and `manifest.json` exist under `./artifacts/outputs/`.

6. **Run model training**
    - Open `D804_PA_Model_TextEmotionNB.ipynb`.
    - Run all cells sequentially.
    - The notebook will:
      - Load `clean.csv` and labels.
      - Train multiple TF–IDF + NB trials.
      - Write metrics to `leaderboard.csv`.
      - Promote the best run to `./artifacts/models/`.

## Outputs and Artifacts

### Global (shared across runs)

- `artifacts/outputs/eda.json` – EDA summary generated during data preparation (class distribution, text stats, token-level observations)
- `artifacts/outputs/label_map.json` – canonical mapping between numeric label IDs and emotion names
- `artifacts/outputs/leaderboard.csv` – master log of all TF–IDF + NB runs, including `run_id` and `test_macro_f1` (used for promotion)
- `artifacts/outputs/manifest.json` – dataset creation metadata (row counts, source, created UTC)

### Per trial (artifacts/trials/<run_id>/)

- `nb.joblib` – trained Multinomial Naive Bayes model for this run
- `tfidf.joblib` – fitted TF–IDF vectorizer for this run
- `metrics.json` – structured metrics output, including `test_macro_f1`
- `classification_report.txt` – human-readable *precision*/*recall*/*F1* per class
- `confusion_matrix_raw.png` – confusion matrix with raw counts
- `confusion_matrix_norm.png` – confusion matrix normalized by support
- `top_missclassifications.csv` – sample-level view of the worst predictions (actual vs predicted), used to diagnose label overlap and noisy text
- `top_words_per_class.json` – analysis of highest-weighted features per class in the TF–IDF space, used to justify model interpretability
- `training_metadata.json` – run metadata (timestamps, params, versions)
- `wordcloud_<label>.png` – one image per numeric label produced for this run

### Promoted model (artifacts/models/)

- `best_nb.joblib` – NB from best run (by `test_macro_f1`)
- `best_tfidf.joblib` – TF–IDF from best run
- `best_metrics.json` – metrics snapshot for the promoted run

## Reproducibility and Design Notes

- **Seeded execution**: fixed random seeds for Python, NumPy, and scikit-learn ensure deterministic data shuffling, vectorization order, and model training results across runs.
- **Stable dataset versioning**: the cleaned dataset (`clean.csv`) and associated artifacts (`manifest.json`, `label_map.json`) are generated once and reused for all trials to maintain consistent input data.
- **Directory auto-creation**: both notebooks validate and create all required paths (`DATA`, `ART`, `TRIALS_ROOT`, `OUTPUTS_DIR`, `MODELS`) before writing, ensuring reliable artifact organization.
- **Metric of record**: `test_macro_f1` serves as the sole promotion criterion for selecting the best model—no fallback or proxy metrics are used.
- **Comprehensive artifact logging**: each trial captures all metrics, confusion matrices, feature-importance summaries, and word-cloud visualizations directly under its own directory for full traceability.
- **Explainability emphasis**: artifacts such as `top_words_per_class.json` and `top_missclassifications.csv` provide insight into feature influence and model behavior, supporting qualitative analysis alongside quantitative metrics.

## License

This project is provided for educational purposes